{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadJafarSiddiq/AnalisisSentimen/blob/main/AnalisisSentimen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNypYVTMXeJh"
      },
      "source": [
        "# **INSTALLASI LIBRARY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ9lbsbRUwnA",
        "outputId": "8645dd05-4884-4126-8ff7-9376d8be28b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.12.14)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=3ca6ecc702dd6143dbeb71f018facfa66743f30843b264b4b4c5707adac71fc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: sastrawi, langdetect, emoji, colorama, vaderSentiment\n",
            "Successfully installed colorama-0.4.6 emoji-2.14.0 langdetect-1.0.9 sastrawi-1.0.1 vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas matplotlib textblob nltk scikit-learn wordcloud seaborn sastrawi langdetect imbalanced-learn openpyxl vaderSentiment colorama numpy emoji joblib nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH1qbYPT7M64"
      },
      "source": [
        "Saat di nltk.download tahap yang pertama ketik d (download), selanjutnya ketik all (download semua package) dan diakhir ketik q (quit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTTTkx9xofpa",
        "outputId": "3bd09046-a363-46df-c3e3-ae18d965ed1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL2BxlfjX05J"
      },
      "source": [
        "# **CRAWLING DATA, PRA-PROCESSING, LABELING DATA, TF-IDF, NAIVE BAYES CLASSIFICATION**\n",
        "\n",
        "Di proses Crawling silahkan salin dan gunakan **Auth Token** yang sudah kami sediakan = cbdb1fe484042df3ce7e4a060fea968f960211d6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGsrO3GyRue-",
        "outputId": "ab35911f-7036-4388-9b99-c26892502632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###############################################################################################################################################################\n",
            "Note: Setiap 600 data yang di dapat harus nunggu 10-15 menit untuk melanjutkan crawl data, karena kena rate limit dari twitter (X)!\n",
            "Note: Jika setelah 600 data ada pesan Eror parsing respone json abaikan saja, karena itu proses menunggu 10-15 menit dari rate limit twitter (X), setelah menunggu 10-15 menit maka crawl data akan dilanjutkan secara otomatis!\n",
            "Note: untuk keyword since dan until dipakai untuk rentang YY-MM-DD dan untuk lang:id untuk bahasa yang akan di crawl datanya, untuk lang:id akan mencari data tweet berbahasa indonesia, sedangkan lang:en data tweet yang berbahasa inggris!\n",
            "###############################################################################################################################################################\n",
            "\u001b[1m\u001b[37mApakah Anda ingin melakukan crawling data dari Twitter + Text Processing? (y/n/o) 'y' (yes), 'n' (langsung text processing), atau 'o' (crawling saja): y\n",
            "\u001b[1m\u001b[37mMasukkan nama file untuk menyimpan hasil crawling (contoh: data): polisi\n",
            "✔ Nama file polisi diterima\n",
            "\u001b[1m\u001b[37mMasukkan kata kunci pencarian Twitter (contoh: 'keyword since:2023-04-01 until:2024-04-01 lang:id'): polisi since:2024-01-01 until:2024-12-29 \n",
            "✔ Kata kunci polisi since:2024-01-01 until:2024-12-29 diterima\n",
            "\u001b[1m\u001b[37mMasukkan jumlah maksimal (limit) yang diambil, min: 1, max: 2000 (contoh: 100): 200\n",
            "✔ Limit 200 diterima\n",
            "\u001b[1m\u001b[37mMasukkan Twitter Auth Token: cbdb1fe484042df3ce7e4a060fea968f960211d6\n",
            "✔ Auth Token cbdb1fe484042df3ce7e4a060fea968f960211d6 diterima\n",
            "✔ Crawling selesai. Data disimpan di tweets-data/polisi.csv\n",
            "✔ Crawling data selesai. File disimpan sebagai polisi.csv\n",
            "===================================================================================\n",
            "Mencari file di: tweets-data/polisi.csv\n",
            "✔ File polisi.csv ditemukan\n",
            "🔔 Menggunakan kolom default: full_text\n",
            "✔ Kolom full_text ditemukan dalam data\n",
            "🔄 Proses sedang berjalan, silahkan tunggu sampai selesai!... |===================================================================================\n",
            "Jumlah Sentimen:\n",
            "----------------\n",
            "Positif: 4 (3.57%)\n",
            "Negatif: 6 (5.36%)\n",
            "Netral: 102 (91.07%)\n",
            "----------------\n",
            "Total Deskripsi: 112\n",
            "Total Persentase: 100.00%\n",
            "===================================================================================\n",
            "===================================================================================\n",
            "Kata-kata yang Penting dan Relevan Berdasarkan Hasil TF-IDF:🔄 Proses sedang berjalan, silahkan tunggu sampai selesai!... \\\n",
            "-------------------------------------------------------------\n",
            "1. polisi: 9.3037\n",
            "2. ya: 3.3160\n",
            "3. laku: 2.2303\n",
            "4. bawa: 2.1568\n",
            "5. biar: 2.0041\n",
            "6. indonesia: 1.9239\n",
            "7. masyarakat: 1.8048\n",
            "8. rakyat: 1.7298\n",
            "🔄 Proses sedang berjalan, silahkan tunggu sampai selesai!... |9. emang: 1.6866\n",
            "10. bunuh: 1.5422\n",
            "-------------------------------------------------------------\n",
            "Menunjukkan relevansi dan signifikansi kata dalam konteks tertentu, berguna untuk mengekstrak kata-kata yang paling penting dalam analisis teks.\n",
            "===================================================================================\n",
            "✔ Proses telah selesai. Hasil telah disimpan ke folder: hasil\n"
          ]
        }
      ],
      "source": [
        "#========================================================================================================================================#\n",
        "#                                                                  Library                                                               #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "import pandas as pd  # Mengimpor library pandas untuk manipulasi data\n",
        "import re  # Mengimpor library re untuk ekspresi reguler\n",
        "import matplotlib.pyplot as plt  # Mengimpor matplotlib untuk membuat grafik\n",
        "from textblob import TextBlob  # Mengimpor TextBlob untuk analisis sentimen\n",
        "from nltk.corpus import stopwords as nltk_stopwords  # Mengimpor stopwords dari NLTK\n",
        "from nltk.tokenize import word_tokenize  # Mengimpor tokenisasi kata dari NLTK\n",
        "from nltk.stem import PorterStemmer  # Mengimpor PorterStemmer dari NLTK\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Mengimpor TfidfVectorizer untuk menghitung TF-IDF\n",
        "from wordcloud import WordCloud  # Mengimpor WordCloud\n",
        "import nltk  # Mengimpor NLTK untuk mengunduh stopwords dan punkt\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Mengimpor CountVectorizer untuk Bag of Words\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans  # Mengimpor KMeans dari sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Mengimpor stemmer dari Sastrawi\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langdetect import detect, DetectorFactory  # Mengimpor deteksi bahasa\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import openpyxl  # Untuk menyimpan hasil ke Excel\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import os\n",
        "import numpy as np\n",
        "# from transformers import pipeline\n",
        "import subprocess  # Mengimpor subprocess untuk menjalankan perintah shell dari Python\n",
        "from colorama import Fore, Style, init\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from imblearn.combine import SMOTETomek\n",
        "import emoji  # Pustaka untuk mendeteksi dan mengganti emoji dengan nama teks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from langdetect import detect, LangDetectException\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Untuk menekan peringatan terkait pembaruan API\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Untuk menekan peringatan umum\n",
        "# Inisialisasi colorama\n",
        "init(autoreset=True)\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                            Buat Folder                                                                 #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Tentukan nama folder untuk menyimpan gambar\n",
        "gambar_folder = 'gambar'\n",
        "# Buat folder jika belum ada\n",
        "os.makedirs(gambar_folder, exist_ok=True)\n",
        "\n",
        "# Tentukan nama folder output\n",
        "output_folder = 'hasil'\n",
        "# Buat folder jika belum ada\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Fungsi untuk menghapus spinner dan membersihkan layar\n",
        "def clear_spinner(message_done):\n",
        "    sys.stdout.write('\\r\\033[K')  # Hapus spinner dari layar\n",
        "    print(f\"{Style.BRIGHT}{Fore.GREEN}{message_done}\")\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                         Fungsi Crawling Data                                                           #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Fungsi untuk melakukan crawling data dari Twitter menggunakan tweet-harvest\n",
        "def crawl_twitter_data(filename, search_keyword, limit, twitter_auth_token):\n",
        "    # Menjalankan perintah npx untuk melakukan crawling\n",
        "    command = f'npx tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}'\n",
        "    subprocess.run(command, shell=True, check=True)  # Menjalankan command di terminal\n",
        "    # Hapus spinner dan tampilkan pesan selesai\n",
        "    clear_spinner(f\"{Style.BRIGHT}{Fore.GREEN}✔ Crawling selesai. {Fore.WHITE}Data disimpan di tweets-data/{filename}\")\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                     Input dan Eksekusi Crawling                                                        #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Input dari pengguna\n",
        "# Mencetak teks dengan warna kuning dan bold\n",
        "print(f\"{Style.BRIGHT}{Fore.GREEN}###############################################################################################################################################################\")\n",
        "print(f\"{Style.BRIGHT}{Fore.YELLOW}Note: {Fore.WHITE}Setiap 600 data yang di dapat harus nunggu 10-15 menit untuk melanjutkan crawl data, karena kena rate limit dari twitter (X)!\")\n",
        "print(f\"{Style.BRIGHT}{Fore.YELLOW}Note: {Fore.WHITE}Jika setelah 600 data ada pesan {Fore.RED}Eror parsing respone json {Fore.WHITE}abaikan saja, karena itu proses menunggu 10-15 menit dari rate limit twitter (X), setelah menunggu 10-15 menit maka crawl data akan dilanjutkan secara otomatis!\")\n",
        "print(f\"{Style.BRIGHT}{Fore.YELLOW}Note: {Fore.WHITE}untuk keyword since dan until dipakai untuk rentang YY-MM-DD dan untuk lang:id untuk bahasa yang akan di crawl datanya, untuk lang:id akan mencari data tweet berbahasa indonesia, sedangkan lang:en data tweet yang berbahasa inggris!\")\n",
        "print(f\"{Style.BRIGHT}{Fore.GREEN}###############################################################################################################################################################\")\n",
        "\n",
        "# Fungsi untuk meminta input dengan validasi tidak kosong khusus untuk nama file\n",
        "def input_file_name_with_check(prompt_text):\n",
        "    while True:\n",
        "        user_input = input(f\"{Style.BRIGHT}{Fore.WHITE}{prompt_text}: \").strip()\n",
        "        if user_input:  # Cek apakah input tidak kosong\n",
        "            print(f\"{Fore.GREEN}✔ Nama file {Fore.WHITE}{user_input} {Fore.GREEN}diterima\")\n",
        "            return user_input  # Kembalikan nilai input jika valid\n",
        "        else:\n",
        "            print(f\"{Fore.RED}✘ {Style.BRIGHT}Input tidak boleh kosong. Silakan coba lagi.\")\n",
        "\n",
        "# Fungsi untuk meminta input dengan validasi tidak kosong untuk kata kunci, limit, dan auth token\n",
        "def input_with_check(prompt_text, keyword):\n",
        "    while True:\n",
        "        user_input = input(f\"{Style.BRIGHT}{Fore.WHITE}{prompt_text}: \").strip()\n",
        "        if user_input:  # Cek apakah input tidak kosong\n",
        "            print(f\"{Fore.GREEN}✔ {keyword} {Fore.WHITE}{user_input} {Fore.GREEN}diterima\")\n",
        "            return user_input  # Kembalikan nilai input jika valid\n",
        "        else:\n",
        "            print(f\"{Fore.RED}✘ {Style.BRIGHT}Input tidak boleh kosong. Silakan coba lagi.\")\n",
        "\n",
        "# Fungsi untuk memastikan nama file memiliki ekstensi .csv\n",
        "def ensure_csv_extension(filename):\n",
        "    if not filename.lower().endswith('.csv'):\n",
        "        filename += '.csv'\n",
        "    return filename\n",
        "\n",
        "# Fungsi untuk meminta input nama file CSV dari folder 'tweets-data' dengan validasi\n",
        "def load_data_from_tweets_data(file_name):\n",
        "    # Tambahkan ekstensi .csv jika belum ada\n",
        "    file_name = ensure_csv_extension(file_name)\n",
        "\n",
        "    # Buat dua kemungkinan nama file: asli dan dengan spasi diganti underscore\n",
        "    file_path_original = os.path.join('tweets-data', file_name)\n",
        "    file_path_alternative = os.path.join('tweets-data', file_name.replace(\" \", \"_\"))\n",
        "\n",
        "    print(f\"{Style.BRIGHT}{Fore.GREEN}===================================================================================\")\n",
        "    print(f\"{Style.BRIGHT}{Fore.WHITE}Mencari file di: {file_path_original}\")\n",
        "\n",
        "    # Coba cari file dengan nama asli terlebih dahulu\n",
        "    if os.path.exists(file_path_original):\n",
        "        print(f\"{Style.BRIGHT}{Fore.GREEN}✔ File {Fore.WHITE}{file_name} {Fore.GREEN}ditemukan\")\n",
        "        return pd.read_csv(file_path_original), True\n",
        "\n",
        "    # Jika nama asli tidak ditemukan, coba nama dengan underscore\n",
        "    print(f\"{Style.BRIGHT}{Fore.YELLOW}⚠ File dengan nama asli tidak ditemukan. Mencoba nama alternatif: {file_path_alternative}\")\n",
        "    if os.path.exists(file_path_alternative):\n",
        "        print(f\"{Style.BRIGHT}{Fore.GREEN}✔ File {Fore.WHITE}{file_name.replace(' ', '_')} {Fore.GREEN}ditemukan\")\n",
        "        return pd.read_csv(file_path_alternative), True\n",
        "\n",
        "    # Jika kedua nama tidak ditemukan, berikan pesan kesalahan\n",
        "    print(f\"{Style.BRIGHT}{Fore.RED}✘ File {Fore.WHITE}{file_name} {Fore.RED}tidak ditemukan di folder 'tweets-data'.\")\n",
        "    return None, False  # Kembalikan None dan status False\n",
        "\n",
        "# Fungsi untuk meminta input nama kolom dengan validasi\n",
        "def get_valid_column(data, default_column=None):\n",
        "    while True:\n",
        "        if default_column:\n",
        "            print(f\"{Style.BRIGHT}{Fore.YELLOW}🔔 Menggunakan kolom default: {Fore.WHITE}{default_column}\")\n",
        "            column_name = default_column\n",
        "        else:\n",
        "            column_name = input(f\"{Style.BRIGHT}{Fore.WHITE}Masukkan nama kolom yang ingin digunakan: \").strip()\n",
        "\n",
        "        if column_name:  # Jika input tidak kosong\n",
        "            if column_name in data.columns:\n",
        "                print(f\"{Style.BRIGHT}{Fore.GREEN}✔ Kolom {Fore.WHITE}{column_name} {Fore.GREEN}ditemukan dalam data\")\n",
        "                return column_name\n",
        "            else:\n",
        "                print(f\"{Style.BRIGHT}{Fore.RED}✘ Kolom {Fore.WHITE}{column_name} {Fore.RED}tidak ditemukan dalam data. {Fore.WHITE}Silakan coba lagi.\")\n",
        "        else:\n",
        "            print(f\"{Style.BRIGHT}{Fore.RED}✘ Input tidak boleh kosong. Silakan coba lagi.\")\n",
        "\n",
        "# Fungsi untuk menanyakan apakah ingin melakukan crawling data\n",
        "def ask_crawling_choice():\n",
        "    while True:\n",
        "        choice = input(f\"{Style.BRIGHT}{Fore.WHITE}Apakah Anda ingin melakukan crawling data dari Twitter + Text Processing? (y/n/o) 'y' (yes), 'n' (langsung text processing), atau 'o' (crawling saja): \").strip().lower()\n",
        "        if choice == \"y\":\n",
        "            return True, False  # Melanjutkan ke pemrosesan setelah crawling\n",
        "        elif choice == \"n\":\n",
        "            return False, False  # Tidak melakukan crawling\n",
        "        elif choice == \"o\":\n",
        "            return True, True  # Hanya melakukan crawling, tidak melanjutkan ke pemrosesan\n",
        "        else:\n",
        "            print(f\"{Style.BRIGHT}{Fore.RED}✘ Pilihan tidak valid. Silakan masukkan 'y' (yes), 'n' (no), atau 'o' (crawling saja).\")\n",
        "\n",
        "# Fungsi untuk spinner berputar\n",
        "def spinning_cursor():\n",
        "    while not done[0]:  # Selama proses belum selesai\n",
        "        for cursor in '|/-\\\\':\n",
        "            yield cursor  # Kembalikan karakter untuk animasi\n",
        "\n",
        "# Variabel untuk kontrol animasi\n",
        "done = [False]\n",
        "spinner = spinning_cursor()\n",
        "\n",
        "# Fungsi untuk menampilkan spinner di dalam pesan\n",
        "def display_spinner_in_message(message):\n",
        "    sys.stdout.write(message)  # Tulis pesan awal\n",
        "    sys.stdout.flush()  # Segera tampilkan pesan\n",
        "    while not done[0]:  # Selama proses belum selesai\n",
        "        cursor = next(spinner)\n",
        "        sys.stdout.write(f'\\r{message} {cursor}')  # Tampilkan cursor di akhir pesan\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.1)  # Jeda singkat\n",
        "    sys.stdout.write('\\r' + ' ' * (len(message) + 2) + '\\r')  # Hapus baris setelah selesai\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# Menanyakan apakah akan melakukan crawling data\n",
        "crawling_choice, skip_processing = ask_crawling_choice()\n",
        "\n",
        "if crawling_choice:\n",
        "    filename = input_file_name_with_check(\"Masukkan nama file untuk menyimpan hasil crawling (contoh: data)\")  # Input dengan underscore\n",
        "    filename = ensure_csv_extension(filename)  # Pastikan ekstensi .csv otomatis ditambahkan\n",
        "\n",
        "    search_keyword = input_with_check(\"Masukkan kata kunci pencarian Twitter (contoh: 'keyword since:2023-04-01 until:2024-04-01 lang:id')\", \"Kata kunci\")\n",
        "    limit = input_with_check(\"Masukkan jumlah maksimal (limit) yang diambil, min: 1, max: 2000 (contoh: 100)\", \"Limit\")\n",
        "    twitter_auth_token = input_with_check(\"Masukkan Twitter Auth Token\", \"Auth Token\")\n",
        "\n",
        "    # Tambahkan pesan bahwa program sedang crawling data\n",
        "    crawling_message = f\"{Style.BRIGHT}{Fore.YELLOW}🔄 Sedang melakukan crawling data...\"\n",
        "\n",
        "    # Memulai thread untuk spinner\n",
        "    spinner_thread = threading.Thread(target=display_spinner_in_message, args=(crawling_message,))\n",
        "    spinner_thread.start()\n",
        "\n",
        "    # Fungsi ini seharusnya merupakan fungsi yang Anda buat untuk crawling\n",
        "    crawl_twitter_data(filename, search_keyword, limit, twitter_auth_token)\n",
        "\n",
        "    # Menandakan bahwa proses telah selesai\n",
        "    done[0] = True\n",
        "    spinner_thread.join()  # Tunggu spinner thread selesai\n",
        "\n",
        "    print(f\"{Style.BRIGHT}{Fore.GREEN}✔ Crawling data selesai. File disimpan sebagai {Fore.WHITE}{filename}\")\n",
        "\n",
        "    # Jika pengguna memilih 'o', program berhenti di sini.\n",
        "    if skip_processing:\n",
        "        print(f\"{Style.BRIGHT}{Fore.YELLOW}✅ Program selesai. Anda dapat memproses data di lain waktu.\")\n",
        "        exit()\n",
        "\n",
        "    # Load data setelah crawling\n",
        "    data, file_found = load_data_from_tweets_data(filename)\n",
        "    if file_found:  # Pastikan file ditemukan\n",
        "        column_name = get_valid_column(data, default_column='full_text')  # Gunakan kolom default 'full_text'\n",
        "\n",
        "        # Tambahkan pesan bahwa program sedang memproses data\n",
        "        processing_message = f\"{Style.BRIGHT}{Fore.YELLOW}🔄 Proses sedang berjalan, silahkan tunggu sampai selesai!...\"\n",
        "\n",
        "        # Memulai thread untuk spinner\n",
        "        done[0] = False  # Reset status done\n",
        "        spinner_thread = threading.Thread(target=display_spinner_in_message, args=(processing_message,))\n",
        "        spinner_thread.start()\n",
        "else:\n",
        "    # Jika pengguna memilih tidak melakukan crawling, minta nama file dari folder 'tweets-data'\n",
        "    while True:\n",
        "        file_name = input_file_name_with_check(f\"Masukkan nama file yang ada di folder 'tweets-data' (contoh: data)\")  # Input dengan underscore\n",
        "        file_name = ensure_csv_extension(file_name)  # Pastikan ekstensi ditambahkan\n",
        "        data, file_found = load_data_from_tweets_data(file_name)\n",
        "\n",
        "        if file_found:  # Jika file ditemukan\n",
        "            column_name = get_valid_column(data)\n",
        "            if column_name:  # Jika kolom valid ditemukan\n",
        "                # Tambahkan pesan bahwa program sedang memproses data\n",
        "                processing_message = f\"{Style.BRIGHT}{Fore.YELLOW}🔄 Proses sedang berjalan, silahkan tunggu sampai selesai!...\"\n",
        "\n",
        "                # Memulai thread untuk spinner\n",
        "                done[0] = False  # Reset status done\n",
        "                spinner_thread = threading.Thread(target=display_spinner_in_message, args=(processing_message,))\n",
        "                spinner_thread.start()\n",
        "                break  # Keluar dari loop jika semua valid\n",
        "        else:\n",
        "            print(f\"{Style.BRIGHT}{Fore.RED}✘ File tidak ditemukan, silakan coba lagi.\")\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                              Pra-Processing                                                            #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# npx tweet-harvest@latest (untuk crawl-twitter)\n",
        "\n",
        "# Menggunakan lambda untuk lebih efisien dalam proses pembersihan teks\n",
        "def clean_and_tokenize(text):\n",
        "    text = str(text)  # Pastikan data diubah menjadi string\n",
        "    if not text:\n",
        "        return 'none'\n",
        "\n",
        "    # Case Folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Emoji Translation\n",
        "    text = translate_emoji(text)\n",
        "\n",
        "    # Text Cleaning\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Hapus URL\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Hapus mention dan hashtag\n",
        "    text = re.sub(r'\\d+', '', text)  # Hapus angka\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Hapus tanda baca\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Hapus spasi berlebih\n",
        "    text = re.sub(r'\\b(\\w+)\\1\\b', r'\\1 \\1', text)  # Pisahkan kata yang sama berurutan\n",
        "\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk membaca kamus dari file Excel\n",
        "def load_dictionary(file_path, sheet_name):\n",
        "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "    return dict(zip(df.iloc[:, 0], df.iloc[:, 1]))  # Mengubah dua kolom menjadi dictionary\n",
        "\n",
        "# Muat kamus dari file Excel\n",
        "file_path = 'kamus.xlsx'  # Ganti dengan path file Excel Anda\n",
        "singkatan_dict = load_dictionary(file_path, 'singkatan')\n",
        "alay_dict = load_dictionary(file_path, 'alay')\n",
        "\n",
        "# Fungsi untuk menerjemahkan emoji ke dalam kata-kata\n",
        "def translate_emoji(text):\n",
        "    if not text:\n",
        "        return text\n",
        "    # Ganti semua emoji dengan nama deskriptifnya\n",
        "    text = emoji.demojize(text)  # Menggunakan emoji.demojize untuk menerjemahkan emoji ke nama teks\n",
        "\n",
        "    emoji_dict = {\n",
        "        ':grinning_face_with_smiling_eyes:': 'senang',\n",
        "        ':smiling_face_with_smiling_eyes:': 'senang',\n",
        "        ':face_with_tears_of_joy:': 'senang',\n",
        "        ':crying_face:': 'sedih',\n",
        "        ':disappointed_face:': 'sedih',\n",
        "        ':angry_face:': 'marah',\n",
        "        ':rage:': 'marah',\n",
        "        ':open_mouth:': 'terkejut',\n",
        "        ':flushed_face:': 'terkejut',\n",
        "        ':heart_eyes:': 'cinta',\n",
        "        ':blush:': 'cinta',\n",
        "    }\n",
        "\n",
        "    for emoji_key, word in emoji_dict.items():\n",
        "        text = text.replace(emoji_key, word)  # Mengganti emoji dengan kata deskriptif\n",
        "\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk mengganti singkatan dan alay\n",
        "def replace_indonesian_terms(text):\n",
        "    if text == 'none' or not text:  # Cek jika teks adalah 'none' atau kosong\n",
        "        return 'none'\n",
        "\n",
        "    try:\n",
        "        lang = detect(text)  # Mendeteksi bahasa dari teks\n",
        "        if lang == 'id':  # Jika bahasa Indonesia\n",
        "            # Implementasi penggantian singkatan dan kata-kata alay\n",
        "            words = text.split()\n",
        "            replaced_words = [singkatan_dict.get(word, alay_dict.get(word, word)) for word in words]\n",
        "            return ' '.join(replaced_words)  # Gabungkan kembali kata-kata\n",
        "    except LangDetectException:\n",
        "        return 'none'  # Kembalikan 'none' jika tidak bisa mendeteksi bahasa\n",
        "\n",
        "    return 'none'  # Kembalikan 'none' jika teks bukan bahasa Indonesia\n",
        "\n",
        "# Menghapus stopwords berdasarkan bahasa\n",
        "def remove_stopwords(tokens, lang):\n",
        "    if tokens == ['none']:  # Jika tokens adalah 'none', kembalikan list ['none']\n",
        "        return ['none']\n",
        "    if not tokens:\n",
        "        return ['none']\n",
        "\n",
        "    if lang == 'en':  # Jika bahasa Inggris\n",
        "        stop_words = set(nltk_stopwords.words('english'))\n",
        "    elif lang == 'id':  # Jika bahasa Indonesia\n",
        "        stop_words = set(nltk_stopwords.words('indonesian'))\n",
        "    else:\n",
        "        stop_words = set()  # Jika bahasa tidak dikenali, tidak ada stopwords\n",
        "\n",
        "    return [word for word in tokens if word not in stop_words] or ['none']\n",
        "\n",
        "# Stemming berdasarkan bahasa\n",
        "def stem_text(tokens, lang):\n",
        "    if tokens == ['none']:  # Jika tokens adalah 'none', kembalikan ['none']\n",
        "        return ['none']\n",
        "    if not tokens:\n",
        "        return ['none']\n",
        "\n",
        "    if lang == 'id':  # Jika bahasa Indonesia\n",
        "        factory = StemmerFactory()\n",
        "        stemmer = factory.create_stemmer()\n",
        "        return [stemmer.stem(word) for word in tokens]\n",
        "    else:  # Jika bahasa Inggris\n",
        "        from nltk.stem import PorterStemmer\n",
        "        stemmer = PorterStemmer()\n",
        "        return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Bersihkan dan proses teks dalam satu langkah\n",
        "data['cleaned'] = data[column_name].apply(lambda x: clean_and_tokenize(x))\n",
        "\n",
        "# Ganti singkatan dan alay hanya untuk teks berbahasa Indonesia\n",
        "data['cleaned'] = data['cleaned'].apply(lambda x: replace_indonesian_terms(x))\n",
        "\n",
        "# Tokenisasi dan deteksi bahasa sekaligus untuk penghapusan stopwords dan stemming\n",
        "data['tokenized'] = data['cleaned'].apply(lambda x: word_tokenize(x) if x != 'none' else ['none'])\n",
        "\n",
        "# Deteksi bahasa sekali untuk semua teks\n",
        "data['lang'] = data['tokenized'].apply(lambda x: detect(' '.join(x)) if x != ['none'] else 'none')\n",
        "\n",
        "# Hapus stopwords berdasarkan bahasa yang terdeteksi\n",
        "data['stopwords'] = data.apply(lambda row: remove_stopwords(row['tokenized'], row['lang']), axis=1)\n",
        "\n",
        "# Lakukan stemming berdasarkan bahasa yang terdeteksi\n",
        "data['stemmed'] = data.apply(lambda row: stem_text(row['stopwords'], row['lang']), axis=1)\n",
        "\n",
        "# Gabungkan kata-kata yang telah di-stem menjadi string\n",
        "data['final'] = data['stemmed'].apply(lambda words: ' '.join(words) if words != ['none'] else 'none')\n",
        "\n",
        "# Hapus duplikat kata\n",
        "data['final_no_duplicates'] = data['final'].apply(lambda x: ' '.join(list(dict.fromkeys(x.split()))) if x != 'none' else 'none')\n",
        "\n",
        "# Hapus baris dengan nilai 'none' di kolom 'final_no_duplicates'\n",
        "data = data[data['final_no_duplicates'] != 'none']\n",
        "\n",
        "# Hapus duplikat berdasarkan kolom 'final_no_duplicates'\n",
        "data = data.drop_duplicates(subset=['final_no_duplicates'], keep='first')\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                       Labeling Data With VADER                                                         #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Inisialisasi analisis VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fungsi untuk analisis sentimen menggunakan VADER\n",
        "def analyze_sentiment_vader(text):\n",
        "    score = analyzer.polarity_scores(text)  # Menganalisis teks\n",
        "    compound_score = score['compound']  # Mengambil nilai compound\n",
        "\n",
        "    if compound_score >= 0.05:  # Jika polaritas positif\n",
        "        return 'Positif'  # Mengembalikan label 'Positif'\n",
        "    elif compound_score <= -0.05:  # Jika polaritas negatif\n",
        "        return 'Negatif'  # Mengembalikan label 'Negatif'\n",
        "    else:\n",
        "        return 'Netral'  # Mengembalikan label 'Netral' jika polaritas mendekati 0\n",
        "\n",
        "# Analisis sentimen\n",
        "data['sentiment'] = data['final_no_duplicates'].apply(analyze_sentiment_vader)  # Menerapkan analisis sentimen ke kolom yang sudah dibersihkan\n",
        "\n",
        "# Hitung jumlah setiap kategori sentimen\n",
        "sentiment_counts = data['sentiment'].value_counts()  # Menghitung jumlah setiap kategori sentimen\n",
        "\n",
        "# Hitung total ulasan\n",
        "total_reviews = sentiment_counts.sum()  # Total jumlah ulasan dari semua kategori\n",
        "\n",
        "# Hitung persentase untuk setiap kategori sentimen\n",
        "sentiment_percentages = (sentiment_counts / total_reviews) * 100  # Menghitung persentase\n",
        "\n",
        "# Menyusun hasil analisis ke dalam DataFrame\n",
        "sentiment_data = pd.DataFrame({\n",
        "    'Sentimen': sentiment_counts.index,\n",
        "    'Jumlah': sentiment_counts.values,\n",
        "    'Persentase (%)': sentiment_percentages.values\n",
        "})\n",
        "\n",
        "# Tentukan path untuk menyimpan file Excel\n",
        "output_excel_path = os.path.join(output_folder, 'sentiment_analysis_results.xlsx')\n",
        "\n",
        "# Simpan hasil ke dalam file Excel\n",
        "sentiment_data.to_excel(output_excel_path, index=False)\n",
        "\n",
        "# Tampilkan jumlah positif, negatif, dan netral di terminal\n",
        "clear_spinner(\"===================================================================================\")\n",
        "clear_spinner(\"Jumlah Sentimen:\")\n",
        "clear_spinner(\"----------------\")\n",
        "clear_spinner(f\"Positif: {sentiment_counts.get('Positif', 0)} ({sentiment_percentages.get('Positif', 0):.2f}%)\")  # Menampilkan jumlah dan persentase sentimen positif\n",
        "clear_spinner(f\"Negatif: {sentiment_counts.get('Negatif', 0)} ({sentiment_percentages.get('Negatif', 0):.2f}%)\")  # Menampilkan jumlah dan persentase sentimen negatif\n",
        "clear_spinner(f\"Netral: {sentiment_counts.get('Netral', 0)} ({sentiment_percentages.get('Netral', 0):.2f}%)\")  # Menampilkan jumlah dan persentase sentimen netral\n",
        "clear_spinner(\"----------------\")\n",
        "clear_spinner(f\"Total Deskripsi: {total_reviews}\")  # Menampilkan total deskripsi\n",
        "\n",
        "# Menampilkan total persentase\n",
        "total_percentage = sentiment_percentages.sum()\n",
        "clear_spinner(f\"Total Persentase: {total_percentage:.2f}%\")  # Menampilkan total persentase\n",
        "clear_spinner(\"===================================================================================\")\n",
        "\n",
        "# Buat diagram batang\n",
        "plt.figure(figsize=(8, 6))  # Menentukan ukuran grafik\n",
        "bars = plt.bar(sentiment_counts.index, sentiment_percentages.values, color=['#66c2a5', '#fc8d62', '#8da0cb'])  # Membuat diagram batang\n",
        "plt.title('Analisis Sentimen')  # Judul grafik\n",
        "plt.xlabel('Sentimen')  # Label sumbu X\n",
        "plt.ylabel('Persentase (%)')  # Label sumbu Y\n",
        "plt.xticks(rotation=45)  # Memutar label sumbu X untuk keterbacaan\n",
        "plt.ylim(0, 100)  # Mengatur batas sumbu Y dari 0 hingga 100\n",
        "plt.grid(axis='y')  # Menambahkan grid pada sumbu Y\n",
        "\n",
        "# Menambahkan persentase di atas batang\n",
        "for i, bar in enumerate(bars):\n",
        "    percentage = sentiment_percentages.iloc[i]  # Menggunakan iloc untuk akses berdasarkan posisi\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
        "            f\"{percentage:.2f}%\", ha='center', va='bottom', fontsize=10)  # Menampilkan persentase di atas batang\n",
        "\n",
        "# Simpan gambar diagram batang ke dalam folder gambar\n",
        "output_sentiment_analysis_path = os.path.join(gambar_folder, 'gambar_sentiment_analysis_vader.png')  # Menentukan path untuk gambar\n",
        "plt.savefig(output_sentiment_analysis_path)  # Menyimpan gambar sebagai file PNG\n",
        "plt.close()  # Menutup grafik\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                                 TF-IDF                                                                 #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Inisialisasi TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Hitung TF-IDF\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['final_no_duplicates'])\n",
        "\n",
        "# Dapatkan fitur nama (kata)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Buat DataFrame untuk hasil TF-IDF dengan dokumen di baris dan kata di kolom\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Transpose DataFrame TF-IDF agar kata-kata menjadi baris dan dokumen menjadi kolom\n",
        "tfidf_dft = tfidf_df.T\n",
        "\n",
        "# Menghitung total nilai TF-IDF untuk setiap kata\n",
        "tfidf_sum = tfidf_df.sum(axis=0)  # Menjumlahkan kolom TF-IDF\n",
        "tfidf_top_words = tfidf_sum.nlargest(10)  # Mengambil 10 kata teratas dengan nilai TF-IDF terbesar\n",
        "\n",
        "# Menampilkan 10 kata teratas dengan penjelasan\n",
        "clear_spinner(\"===================================================================================\")\n",
        "clear_spinner(\"Kata-kata yang Penting dan Relevan Berdasarkan Hasil TF-IDF:\")\n",
        "clear_spinner(\"-------------------------------------------------------------\")\n",
        "for i, (word, value) in enumerate(zip(tfidf_top_words.index, tfidf_top_words.values), start=1):\n",
        "    clear_spinner(f\"{i}. {word}: {value:.4f}\") # Menampilkan nomor, kata, dan nilainya\n",
        "clear_spinner(\"-------------------------------------------------------------\")\n",
        "clear_spinner(\"Menunjukkan relevansi dan signifikansi kata dalam konteks tertentu, berguna untuk mengekstrak kata-kata yang paling penting dalam analisis teks.\")\n",
        "clear_spinner(\"===================================================================================\")\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                            Visualisasi TF-IDF                                                          #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Visualisasi hasil TF-IDF menggunakan diagram batang\n",
        "plt.figure(figsize=(10, 6))\n",
        "tfidf_top_words.plot(kind='barh', color='skyblue')\n",
        "\n",
        "# Menambahkan judul dan label sumbu\n",
        "plt.title('10 Kata Teratas Berdasarkan Nilai TF-IDF', fontsize=16)\n",
        "plt.xlabel('Nilai TF-IDF', fontsize=14)\n",
        "plt.ylabel('Kata', fontsize=14)\n",
        "plt.gca().invert_yaxis()  # Membalik sumbu y agar kata teratas muncul di atas\n",
        "\n",
        "# Menambahkan nilai TF-IDF di sebelah kanan batang\n",
        "for index, value in enumerate(tfidf_top_words.values):\n",
        "    plt.text(value + 0.05, index, f'{value:.4f}', va='center')  # Menambahkan offset pada value untuk memberikan jarak dari batang\n",
        "\n",
        "# Simpan gambar diagram batang ke dalam folder gambar\n",
        "output_tfidf_visualization_path = os.path.join(gambar_folder, 'gambar_tfidf_visualization.png')  # Menentukan path untuk gambar\n",
        "plt.savefig(output_tfidf_visualization_path)  # Menyimpan gambar sebagai file PNG\n",
        "# Menutup diagram\n",
        "plt.close()\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                         Word Cloud TF-IDF                                                              #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "def plot_wordcloud(tfidf_matrix, feature_names):\n",
        "    # Menghitung total nilai TF-IDF untuk setiap kata\n",
        "    tfidf_sum = tfidf_matrix.sum(axis=0).A1  # Menjumlahkan kolom TF-IDF\n",
        "    word_tfidf = dict(zip(feature_names, tfidf_sum))  # Menggabungkan kata dan nilai TF-IDF dalam dictionary\n",
        "\n",
        "    # Membuat Word Cloud dengan semua kata\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_tfidf)\n",
        "\n",
        "    # Tampilkan Word Cloud\n",
        "    plt.figure(figsize=(10, 5))  # Menentukan ukuran grafik\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')  # Menampilkan Word Cloud\n",
        "    plt.axis('off')  # Menghilangkan sumbu\n",
        "    plt.title('Word Cloud dari Semua Kata TF-IDF')  # Judul grafik\n",
        "    # Simpan gambar Word Cloud ke dalam folder gambar\n",
        "    output_wordcloud_tfidf_path = os.path.join(gambar_folder, 'gambar_wordcloud_tfidf.png')  # Menentukan path untuk gambar Word Cloud TF-IDF\n",
        "    plt.savefig(output_wordcloud_tfidf_path)  # Menyimpan gambar Word Cloud sebagai file PNG\n",
        "    plt.close()  # Menutup grafik\n",
        "\n",
        "# Panggil fungsi untuk memplot Word Cloud dengan semua kata\n",
        "plot_wordcloud(tfidf_matrix, feature_names)\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                            Naive Bayes Classification                                                  #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Mengambil kolom fitur (TF-IDF) dan target (sentimen)\n",
        "X = tfidf_matrix  # Matriks TF-IDF\n",
        "y = data['sentiment']  # Target variabel (sentimen: Positif, Negatif, Netral)\n",
        "\n",
        "# Visualisasi distribusi kelas awal\n",
        "plt.figure(figsize=(6, 4))\n",
        "y.value_counts().plot(kind='bar', color=['#66c2a5', '#fc8d62', '#8da0cb'])\n",
        "plt.title('Distribusi Kelas Awal')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(gambar_folder, 'gambar_distribusi_kelas_awal.png'))\n",
        "plt.close()\n",
        "\n",
        "# Membagi data menjadi data latih dan data uji dengan stratifikasi\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Terapkan RandomOverSampler untuk mengatasi ketidakseimbangan kelas di data latih\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Visualisasi distribusi kelas setelah resampling\n",
        "plt.figure(figsize=(6, 4))\n",
        "pd.Series(y_train_resampled).value_counts().plot(kind='bar', color=['#66c2a5', '#fc8d62', '#8da0cb'])\n",
        "plt.title('Distribusi Kelas Setelah Resampling')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(gambar_folder, 'gambar_distribusi_kelas_resampling.png'))\n",
        "plt.close()\n",
        "\n",
        "# Inisialisasi model Naive Bayes Multinomial\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Latih model dengan data latih yang sudah diseimbangkan\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Prediksi menggunakan data uji\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluasi hasil\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "\n",
        "# Visualisasi akurasi\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(['Akurasi'], [accuracy], color='#fc8d62')\n",
        "plt.title('Akurasi Model Naive Bayes')\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Nilai Akurasi')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Menambahkan persentase di atas diagram batang\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height,\n",
        "             f\"{height * 100:.2f}%\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Simpan visualisasi ke file\n",
        "plt.savefig(os.path.join(gambar_folder, 'gambar_akurasi_model.png'))\n",
        "plt.close()\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                          Confusion Matrix                                                              #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Menghitung confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Menyimpan confusion matrix ke file Excel\n",
        "cm_df = pd.DataFrame(cm, index=['Positif', 'Negatif', 'Netral'], columns=['Positif', 'Negatif', 'Netral'])\n",
        "\n",
        "# Simpan confusion matrix ke dalam file Excel\n",
        "output_cm_excel_path = os.path.join(output_folder, 'hasil_confusion_matrix.xlsx')\n",
        "with pd.ExcelWriter(output_cm_excel_path) as writer:\n",
        "    cm_df.to_excel(writer, sheet_name='Confusion Matrix')\n",
        "\n",
        "# Visualisasi confusion matrix dengan heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n",
        "            xticklabels=['Positif', 'Negatif', 'Netral'],\n",
        "            yticklabels=['Positif', 'Negatif', 'Netral'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Aktual')\n",
        "\n",
        "output_cm_path = os.path.join(gambar_folder, 'gambar_confusion_matrix.png')\n",
        "plt.savefig(output_cm_path)\n",
        "plt.close()\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                          Classification Report - Visualisasi                                           #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Mengubah classification report menjadi DataFrame\n",
        "report_df = pd.DataFrame.from_dict(classification_report(y_test, y_pred, output_dict=True)).T\n",
        "\n",
        "# Visualisasi classification report (termasuk weighted avg dan accuracy)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(report_df.astype(float), annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Classification Report Heatmap')\n",
        "plt.ylabel('Kelas')\n",
        "plt.xlabel('Metode Evaluasi')\n",
        "\n",
        "# Simpan gambar Classification Report\n",
        "output_report_path = os.path.join(gambar_folder, 'gambar_classification_report_heatmap.png')\n",
        "plt.savefig(output_report_path)\n",
        "plt.close()\n",
        "\n",
        "# Mengubah classification report menjadi DataFrame\n",
        "classification_report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Simpan classification report ke dalam file Excel\n",
        "output_report_excel_path = os.path.join(output_folder, 'hasil_classification_report.xlsx')\n",
        "classification_report_df.to_excel(output_report_excel_path)\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                            Visualisasi Hasil Klasifikasi                                               #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Visualisasi distribusi sentimen yang diprediksi dengan grafik batang\n",
        "predicted_sentiments = pd.Series(y_pred).value_counts()\n",
        "predicted_sentiment_percentages = (predicted_sentiments / predicted_sentiments.sum()) * 100\n",
        "\n",
        "# Membuat diagram batang hasil prediksi\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(predicted_sentiments.index, predicted_sentiment_percentages.values, color=['#66c2a5', '#fc8d62', '#8da0cb'])\n",
        "plt.title('Hasil Analisis Sentimen')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Persentase (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Menambahkan jumlah dan persentase di atas batang\n",
        "for i, bar in enumerate(bars):\n",
        "    count = predicted_sentiments.iloc[i]\n",
        "    percentage = predicted_sentiment_percentages.iloc[i]\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        bar.get_height(),\n",
        "        f\"{count} ({percentage:.2f}%)\",\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=10,\n",
        "        color='black'\n",
        "    )\n",
        "\n",
        "# Simpan gambar hasil klasifikasi\n",
        "output_classification_path = os.path.join(gambar_folder, 'gambar_klasifikasi_naive_bayes.png')\n",
        "plt.savefig(output_classification_path)\n",
        "plt.close()\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                                Save to                                                                 #\n",
        "#========================================================================================================================================#\n",
        "\n",
        "# Simpan hasil analisis sentimen ke file Excel\n",
        "output_sentiment_file_path = os.path.join(output_folder, 'hasil_analisis_sentimen.xlsx')\n",
        "data_reset = data.reset_index(drop=True)  # Reset index tanpa menyimpan index lama\n",
        "data_reset.index += 1  # Tambahkan 1 ke setiap index agar mulai dari 1\n",
        "data_reset.to_excel(output_sentiment_file_path, index=True)  # Menyimpan DataFrame analisis sentimen ke file Excel\n",
        "\n",
        "# Simpan hasil TF-IDF ke file Excel\n",
        "output_tfidf_file_path = os.path.join(output_folder, 'hasil_tfidf.csv')  # Menentukan path untuk file output TF-IDF\n",
        "tfidf_dft.columns = range(1, len(tfidf_dft.columns) + 1)  # Indeks kolom mulai dari 1\n",
        "tfidf_dft.to_csv(output_tfidf_file_path, index=True)  # Menyimpan DataFrame TF-IDF ke file CSV\n",
        "\n",
        "# Menandakan bahwa proses telah selesai\n",
        "done[0] = True\n",
        "spinner_thread.join()  # Tunggu spinner thread selesai\n",
        "\n",
        "# Menandakan bahwa proses text processing telah selesai\n",
        "print(f\"{Style.BRIGHT}{Fore.GREEN}✔ Proses telah selesai. Hasil telah disimpan ke folder: {output_folder}\")\n",
        "\n",
        "#========================================================================================================================================#\n",
        "#                                                                  Selesai                                                               #\n",
        "#========================================================================================================================================#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pNypYVTMXeJh",
        "EL2BxlfjX05J"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}